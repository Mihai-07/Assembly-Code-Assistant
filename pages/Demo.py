from langchain_openai import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.prompts import PromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
import os
import streamlit as st

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs):
        self.text += token
        self.container.markdown(self.text + "▌ ")

os.environ["DEEPSEEK_API_KEY"] = st.secrets["api_keys"]["deepseek"]

format_docs = lambda docs: "\n\n".join(doc.page_content for doc in docs)

def call_model(question: str, chat_history: str, container, handler):
    llm = ChatOpenAI(
        model="deepseek/deepseek-chat-v3-0324:free",
        temperature=0,
        streaming=True,
        callbacks=[handler],
        base_url="https://openrouter.ai/api/v1",
        api_key=os.getenv("DEEPSEEK_API_KEY")
    )

    template = """
    Given the chat history and the context below, answer the following question:

    Chat History: {chat_history}
    Context: {context}
    Question: {question}
    """
    
    prompt = PromptTemplate(
        input_variables=["context", "question", "chat_history"],
        template=template
    )

    chain = prompt | llm

    store = FAISS.load_local(
        folder_path="x86-asm-docs",
        embeddings=SentenceTransformerEmbeddings(),
        allow_dangerous_deserialization=True,
    )

    retriever = store.as_retriever(search_kwargs={"k": 3})

    context = format_docs(
        retriever.invoke(question)
    )

    return chain.invoke({
        "chat_history": chat_history,
        "context": context,
        "question": question
    }).content

def format_chat_history(chat_history):
    return \
        "\n".join([f"Role: {turn[0]}\nMessage: {turn[1]}" for turn in chat_history])

question = st.chat_input("Ask me anything")

if question is None:
    pass
elif question.strip() == "":
    st.error("❌ Please type something here!")
else:
    try:
        if "chat_history" not in st.session_state:
            st.session_state["chat_history"] = []

        if len(st.session_state["chat_history"]):
            for turn in st.session_state["chat_history"]:
                with st.chat_message(turn[0]):
                    st.markdown(turn[1])

        with st.spinner("⬇️ Retrieving information and generating response..."):
            with st.chat_message("User"):
                st.write(question)

            with st.chat_message("Assistant"):
                container = st.empty()
                handler = StreamHandler(container=container)

                result = call_model(
                    question=question,
                    chat_history=format_chat_history(st.session_state["chat_history"]),
                    container=container,
                    handler=handler
                )

            st.warning("⚠️ This response is generated by AI and may contain inaccuracies. Use at your own risk.")

            st.session_state["chat_history"].append(("User", question))
            st.session_state["chat_history"].append(("Assistant", result))
    except Exception as e:
        st.error("❌️ An error has occured! Please try again later.")
        print(e)
